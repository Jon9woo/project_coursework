{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#웹드라이버 설정\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "options.add_experimental_option(\"useAutomationExtension\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConnectionError방지\n",
    "headers = { \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/98.0.4758.102\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#크롤링시 필요한 라이브러리 불러오기\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "#웹드라이버 설정\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "options.add_experimental_option(\"useAutomationExtension\", False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "['2023.04.01']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "start_date='20230401'\n",
    "end_date='20230401'\n",
    "date_list=pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "date_list_str =[]\n",
    "# 평일만 추출 시 weekday() <= 4, \"7\"일때는 모든 날짜\n",
    "for i in range(len(date_list)):\n",
    "    if date_list[i].weekday()<=7:    \n",
    "        s = str(date_list[i])\n",
    "        s = s[0:10].replace('-','.')\n",
    "        date_list_str.append(s)\n",
    "\n",
    "# 생성된 날짜 확인\n",
    "print(len(date_list_str))\n",
    "print(date_list_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 페이지 url 형식에 맞게 바꾸어 주는 함수 만들기\n",
    "  #입력된 수를 1, 11, 21, 31 ...만들어 주는 함수\n",
    "def makePgNum(num):\n",
    "    if num == 1:\n",
    "        return num\n",
    "    elif num == 0:\n",
    "        return num+1\n",
    "    else:\n",
    "        return num+9*(num-1)\n",
    "\n",
    "\n",
    "# 날짜 없이 크롤링할 url 생성하는 함수 만들기(검색어, 크롤링 시작 페이지, 크롤링 종료 페이지)\n",
    "def makeUrl(search,start_pg,end_pg):\n",
    "    if start_pg == end_pg:\n",
    "        start_page = makePgNum(start_pg)\n",
    "        url = \"https://search.naver.com/search.naver?where=news&sm=tab_pge&query=\" + search + \"&start=\" + str(start_page) \n",
    "        print(\"생성url: \",url)\n",
    "        return url\n",
    "    else:\n",
    "        urls= []\n",
    "        for i in range(start_pg,end_pg+1):\n",
    "            page = makePgNum(i)\n",
    "            url = \"https://search.naver.com/search.naver?where=news&sm=tab_pge&query=\" + search + \"&start=\" + str(page)\n",
    "            urls.append(url)\n",
    "        print(\"생성url: \",urls)\n",
    "        return urls\n",
    "    \n",
    "    \n",
    "\n",
    "# 날짜 있이 크롤링할 url 생성하는 함수 만들기(검색어, 크롤링 시작 페이지, 크롤링 종료 페이지)\n",
    "def makeUrl(search,date,start_pg,end_pg):\n",
    "    if start_pg == end_pg:\n",
    "        start_page = makePgNum(start_pg) \n",
    "        url = \"https://search.naver.com/search.naver?where=news&sm=tab_pge&query=\" + search + \"&sort=0&photo=0&field=0&pd=3&ds=\"+ date + \"&de=\" + date + \"&cluster_rank=82&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so:r,p:from20221218to20221218,a:all&start=\" + str(start_page)\n",
    "        print(\"생성url: \",url)\n",
    "        return url\n",
    "    else:\n",
    "        urls= []\n",
    "        for i in range(start_pg,end_pg+1):\n",
    "            page = makePgNum(i)\n",
    "            url = \"https://search.naver.com/search.naver?where=news&sm=tab_pge&query=\" + search + \"&sort=0&photo=0&field=0&pd=3&ds=\"+ date + \"&de=\" + date + \"&cluster_rank=82&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so:r,p:from20221218to20221218,a:all&start=\" + str(page)\n",
    "            urls.append(url)\n",
    "        print(\"생성url: \",urls)\n",
    "        return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생성url:  https://search.naver.com/search.naver?where=news&sm=tab_pge&query=인공지능&sort=0&photo=0&field=0&pd=3&ds=2023.04.01&de=2023.04.01&cluster_rank=82&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so:r,p:from20221218to20221218,a:all&start=1\n"
     ]
    }
   ],
   "source": [
    "##########뉴스크롤링 시작###################\n",
    "search_list = ['인공지능']\n",
    "search_urls = []\n",
    "for search in search_list:\n",
    "    for date in date_list_str:\n",
    "        page=1\n",
    "        page2=1\n",
    "        #검색 시작할 페이지 입력\n",
    "        #page = int(input(\"\\n크롤링할 시작 페이지를 입력해주세요. ex)1(숫자만입력):\")) # ex)1 =1페이지,2=2페이지...\n",
    "        #print(\"\\n크롤링할 시작 페이지: \",page,\"페이지\")   \n",
    "        #검색 종료할 페이지 입력\n",
    "        #page2 = int(input(\"\\n크롤링할 종료 페이지를 입력해주세요. ex)1(숫자만입력):\")) # ex)1 =1페이지,2=2페이지...\n",
    "        #print(\"\\n크롤링할 종료 페이지: \",page2,\"페이지\")\n",
    "\n",
    "        #검색할 날짜 지정\n",
    "        #date = input(\"\\n크롤링할 날짜를 입력해주세요. ex)2022.12.16(해당양식입력):\") # ex)1 =1페이지,2=2페이지...\n",
    "        # naver url 생성\n",
    "        t = makeUrl(search,date,page,page2)\n",
    "        search_urls.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jongwoom1pro/opt/anaconda3/envs/dan1/lib/python3.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: use options instead of chrome_options\n",
      "  if __name__ == \"__main__\":\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.mbn.co.kr/\n",
      "https://n.news.naver.com/mnews/article/057/0001732729?sid=104\n",
      "https://magazine.hankyung.com/\n",
      "https://n.news.naver.com/mnews/article/050/0000064617?sid=101\n",
      "http://www.bizwatch.co.kr/\n",
      "https://n.news.naver.com/mnews/article/648/0000015171?sid=105\n",
      "https://www.boannews.com/Default.asp\n",
      "https://theguru.co.kr/index.html\n",
      "https://www.cwn.kr/\n",
      "https://www.edaily.co.kr/\n",
      "https://www.edaily.co.kr/popup/viewpopup.html?v=3\n",
      "https://n.news.naver.com/mnews/article/018/0005454123?sid=105\n",
      "defaultdict(<class 'str'>, {'https://n.news.naver.com/mnews/article/057/0001732729?sid=104': '인공지능', 'https://n.news.naver.com/mnews/article/050/0000064617?sid=101': '인공지능', 'https://n.news.naver.com/mnews/article/648/0000015171?sid=105': '인공지능', 'https://n.news.naver.com/mnews/article/018/0005454123?sid=105': '인공지능'})\n"
     ]
    }
   ],
   "source": [
    "## selenium으로 navernews만 뽑아오기##\n",
    "# 버전에 상관 없이 os에 설치된 크롬 브라우저 사용\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "# headless 설정은 백그라운드로 돌릴거면 주석 해제, 주석 시 크롤링 모습 보여줌.\n",
    "#chrome_options.add_argument('--headless')\n",
    "#chrome_options.add_argument('--no-sandbox')\n",
    "#chrome_options.add_argument(\"--single-process\")\n",
    "#chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install(), chrome_options=chrome_options)\n",
    "driver.implicitly_wait(3)\n",
    "\n",
    "from collections import defaultdict\n",
    "d = defaultdict(str)\n",
    "\n",
    "\n",
    "# selenium으로 검색 페이지 불러오기 #\n",
    "\n",
    "naver_urls=[]\n",
    "\n",
    "for i in search_urls:\n",
    "    driver.get(i)\n",
    "    time.sleep(1) #대기시간 변경 가능\n",
    "    \n",
    "    #정규표현식으로 해당 url에서 검색어 추출 (종목 추출)\n",
    "    name = re.search('(?<=query=)\\w+(?=&sort)', i).group()\n",
    "\n",
    "    # 네이버 기사 눌러서 제목 및 본문 가져오기#\n",
    "    # 네이버 기사가 있는 기사 css selector 모아오기\n",
    "    a = driver.find_elements(By.CSS_SELECTOR,'a.info')\n",
    "    try:\n",
    "        # 위에서 생성한 css selector list 하나씩 클릭하여 본문 url얻기\n",
    "        for i in a:\n",
    "            try:\n",
    "                i.click()\n",
    "\n",
    "                # 현재탭에 접근\n",
    "                driver.switch_to.window(driver.window_handles[1])\n",
    "                time.sleep(3) #대기시간 변경 가능\n",
    "\n",
    "                # 네이버 뉴스 url만 가져오기\n",
    "\n",
    "                url = driver.current_url\n",
    "                print(url)\n",
    "\n",
    "                if \"news.naver.com\" in url:\n",
    "                    # naver_urls.append(url)\n",
    "                    d[url] = name\n",
    "    \n",
    "                else:\n",
    "                    pass\n",
    "                \n",
    "                # 현재 탭 닫기\n",
    "                driver.close()\n",
    "\n",
    "                # 다시처음 탭으로 돌아가기(매우 중요!!!)\n",
    "                driver.switch_to.window(driver.window_handles[0])\n",
    "            except:\n",
    "                pass\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "driver.quit() # 모든 탭 끄는거\n",
    "# print(f'해당 페이지 네이버 뉴스 개수는 {len(naver_urls)}개, 주소 리스트는{naver_urls}')\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "naver_urls = list(d.keys())\n",
    "stock = list(d.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title의 길이는 4\n",
      "dates의 길이는 4\n",
      "contents의 길이는 4\n",
      "stocks의 길이는 4\n"
     ]
    }
   ],
   "source": [
    "###naver 기사 본문 및 제목 가져오기###\n",
    "stocks =[]\n",
    "titles = []\n",
    "contents=[]\n",
    "dates=[]\n",
    "\n",
    "# ConnectionError방지\n",
    "headers = { \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/98.0.4758.102\" }\n",
    "\n",
    "for i in range(len(naver_urls)):\n",
    "    original_html = requests.get(naver_urls[i],headers=headers)\n",
    "    html = BeautifulSoup(original_html.text, \"html.parser\")\n",
    "    # 검색결과확인시\n",
    "    #print(html)\n",
    "    \n",
    "    #종목명 가져오기\n",
    "    stocks.append(stock[i])\n",
    "    \n",
    "    #뉴스 제목 가져오기\n",
    "    title = html.select(\"#ct > div.media_end_head.go_trans > div.media_end_head_title > h2\") # F12에서 제목 파트 클릭 후 우클릭 > copy > copy selector 눌러서 나오는 정보 가져오면 됨.\n",
    "    # list합치기\n",
    "    title = ''.join(str(title))\n",
    "    # html태그제거\n",
    "    pattern1 = '<[^>]*>'\n",
    "    title = re.sub(pattern=pattern1,repl='',string=title)\n",
    "    titles.append(title)\n",
    "    \n",
    "    #뉴스 날짜 가져오기\n",
    "    date = html.select(\"#ct > div.media_end_head.go_trans > div.media_end_head_info.nv_notrans > div.media_end_head_info_datestamp > div > span\")\n",
    "    try: #뉴스 날짜가 없는 경우가 있어서 try except로 처리\n",
    "        date = re.search('date-time=\"(.+)\"',str(date[0])).groups()[0] #정규표현식, 동호가 알려줌.\n",
    "        dates.append(date) #날짜가 있으면 dates에 추가\n",
    "    except:\n",
    "        dates.append('None') #날짜가 없으면 dates에 None 추가\n",
    "\n",
    "    #뉴스 본문 가져오기\n",
    "\n",
    "    content = html.select(\"#dic_area\") # F12에서 본문 파트 클릭 후 우클릭 > copy > copy selector 눌러서 나오는 정보 가져오면 됨.\n",
    "\n",
    "    # 기사 텍스트만 가져오기\n",
    "    # list합치기\n",
    "    content = ''.join(str(content))\n",
    "    \n",
    "    #html태그제거 및 텍스트 다듬기\n",
    "    content = re.sub(pattern=pattern1,repl='',string=content)\n",
    "    pattern2 = \"\"\"[\\n\\n\\n\\n\\n// flash 오류를 우회하기 위한 함수 추가\\nfunction _flash_removeCallback() {}\"\"\"\n",
    "    content = content.replace(pattern2,'')\n",
    "\n",
    "    contents.append(content)\n",
    "    \n",
    "    #\n",
    "\n",
    "'''for i in range(2):\n",
    "    print(titles[i])\n",
    "    print(contents[i])'''\n",
    "print('title의 길이는',len(titles))\n",
    "print('dates의 길이는',len(dates))\n",
    "print('contents의 길이는',len(contents))\n",
    "print('stocks의 길이는',len(stocks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#df는 11월달\n",
    "df12 = pd.DataFrame({'title':titles, 'date':dates, 'content':contents, 'name':stocks, 'url':naver_urls})\n",
    "df12.to_csv('/Users/jongwoom1pro/Coding/text_magician/news_crawling_임시.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>name</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['AI 기술 오류'로 절도범 누명 써...인공지능 믿어도 될까?]</td>\n",
       "      <td>2023-04-01 16:20:47</td>\n",
       "      <td>[\\n안면인식 알고리즘 기술 오류로 무고한 남성 체포돼인공지능(AI)을 이용한 안면...</td>\n",
       "      <td>인공지능</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/057/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[EDITOR's LETTER] 챗GPT 시대, 인공지능은 당신의 질문을 평가한다!]</td>\n",
       "      <td>2023-04-01 06:02:30</td>\n",
       "      <td>[\\n\\n\\n\\n\\n 오래전 들은 한 친구의 경험담입니다. 충청남도 어딘가에서 식당...</td>\n",
       "      <td>인공지능</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/050/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[인공지능이 기지국 관리하는 시대]</td>\n",
       "      <td>2023-04-01 09:31:01</td>\n",
       "      <td>[\\n[위클리 잇(IT)슈]네이버페이, 삼성페이 오프라인 결제 지원웨이브, 2023...</td>\n",
       "      <td>인공지능</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/648/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[이탈리아서 차단?…챗GPT는 데이터를 어디서 얻었나[궁금한AI]]</td>\n",
       "      <td>2023-04-01 10:13:01</td>\n",
       "      <td>[\\n이탈리아 챗GPT 학습 위해 정보 수집·저장 근거 없다대책 마련할 때까지 일시...</td>\n",
       "      <td>인공지능</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/018/000...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              title                 date  \\\n",
       "0             ['AI 기술 오류'로 절도범 누명 써...인공지능 믿어도 될까?]  2023-04-01 16:20:47   \n",
       "1  [[EDITOR's LETTER] 챗GPT 시대, 인공지능은 당신의 질문을 평가한다!]  2023-04-01 06:02:30   \n",
       "2                               [인공지능이 기지국 관리하는 시대]  2023-04-01 09:31:01   \n",
       "3             [이탈리아서 차단?…챗GPT는 데이터를 어디서 얻었나[궁금한AI]]  2023-04-01 10:13:01   \n",
       "\n",
       "                                             content  name  \\\n",
       "0  [\\n안면인식 알고리즘 기술 오류로 무고한 남성 체포돼인공지능(AI)을 이용한 안면...  인공지능   \n",
       "1  [\\n\\n\\n\\n\\n 오래전 들은 한 친구의 경험담입니다. 충청남도 어딘가에서 식당...  인공지능   \n",
       "2  [\\n[위클리 잇(IT)슈]네이버페이, 삼성페이 오프라인 결제 지원웨이브, 2023...  인공지능   \n",
       "3  [\\n이탈리아 챗GPT 학습 위해 정보 수집·저장 근거 없다대책 마련할 때까지 일시...  인공지능   \n",
       "\n",
       "                                                 url  \n",
       "0  https://n.news.naver.com/mnews/article/057/000...  \n",
       "1  https://n.news.naver.com/mnews/article/050/000...  \n",
       "2  https://n.news.naver.com/mnews/article/648/000...  \n",
       "3  https://n.news.naver.com/mnews/article/018/000...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dan1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
